{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "JRb5NV1K5Wta"
      },
      "source": [
        "# More Corpus fun with NLTK\n",
        "\n",
        "16 Feb 2022"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0H_X8U65Wtk"
      },
      "source": [
        "## Let's load one of NLTK's corpora\n",
        "\n",
        "NLTK can read in an entire corpus from a directory (the “root” directory).\n",
        "\n",
        "As it reads in a corpus, it applies word tokenization: `.words()` and sentence tokenization: `.sents()`. \n",
        "\n",
        "I downloaded the inagural addresses corpus from here, which is what we'll be working on today: http://www.nltk.org/nltk_data/. I'm also putting it on our github.\n",
        "\n",
        "Load these into your files directory (on the tab at the left)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nq2Zhvp35Wtk"
      },
      "outputs": [],
      "source": [
        "import nltk \n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import PlaintextCorpusReader\n",
        "corpus_root = '/content/inagural/'  \n",
        "inaug = PlaintextCorpusReader(corpus_root, '.*txt')  # all files ending in 'txt' "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then let's print out the list of the files"
      ],
      "metadata": {
        "id": "6yrEgH8Q7T8E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1M8_5s2I5Wtl"
      },
      "outputs": [],
      "source": [
        "%pprint  # turn off pretty printing, which prints too many lines\n",
        "# .txt file names as file IDs\n",
        "inaug.fileids()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK loads all these into one database, and we can look at the first 50 words of our corpus. Since they're stored in order, these are the first 50 words of Washington's inagural address. "
      ],
      "metadata": {
        "id": "b8ByJgDn7Yos"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9WXynMj5Wtm"
      },
      "outputs": [],
      "source": [
        "# NLTK automatically tokenizes the corpus for us. First 50 words: \n",
        "inaug.words()[:50]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you know which file you're looking for, you can specify the ID, and look into files that way too:"
      ],
      "metadata": {
        "id": "aJ0XjuII7sMS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UM5cX3_P5Wtn"
      },
      "outputs": [],
      "source": [
        "# First 50 words from Obama 2013:\n",
        "inaug.words('2013-Obama.txt')[:50]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK automatically segments sentences too, which are accessed through .sents() -- you do have to download the punkt package to do this, but we've specified this already."
      ],
      "metadata": {
        "id": "x9sy_1tB8EdX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zwid1Bbt5Wtn"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(inaug.sents('2013-Obama.txt')[0])   # first sentence\n",
        "print(inaug.sents('2013-Obama.txt')[1])   # 2nd sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So let's do some comparison across these presidents."
      ],
      "metadata": {
        "id": "1Z46qHI78O76"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJcqSB7E5Wtn"
      },
      "outputs": [],
      "source": [
        "# How long are these speeches in terms of word and sentence count?\n",
        "print('Washington -- Word count: ', len(inaug.words('1789-Washington.txt')), 'Sentence count: ', len(inaug.sents('1789-Washington.txt')))\n",
        "print('Obama -- Word count:', len(inaug.words('2013-Obama.txt')), 'Sentence count: ', len(inaug.sents('2013-Obama.txt')))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can write a loop that will give us these stats for all the presidents & their speeches"
      ],
      "metadata": {
        "id": "wozHU0Ph8x22"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIPR36rx5Wto"
      },
      "outputs": [],
      "source": [
        "# for-loop through file IDs and print out various stats. \n",
        "# While looping, populate fid_avsent which holds avg sent lengths.\n",
        "\n",
        "speech_stats = []    # initialize an empty list\n",
        "\n",
        "for file in inaug.fileids():\n",
        "    wcount = len(inaug.words(file))\n",
        "    scount = len(inaug.sents(file))\n",
        "    print(f'Words: {wcount} Sents: {scount} Avg. w/s: {round(wcount/scount)} {file}', sep='\\t')  \n",
        "    speech_stats.append( (wcount/scount, file) )      # append a pair (x, y) to list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuCmTbmX5Wto"
      },
      "source": [
        "\n",
        "### OH NO! AN ERROR!\n",
        "\n",
        "Look at the error output -- the 2005-Bush.txt file produces a **Unicode encoding error**. \n",
        "\n",
        "WHAT CAN WE DO??\n",
        "\n",
        "http://www.presidency.ucsb.edu/inaugurals.php\n",
        "\n",
        "Ideas: \n",
        "- save a new copy of the file, making sure it's in utf-8 format\n",
        "- use regex to find & replace all of the errors (if we can understand them)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKPg4PD05Wtp"
      },
      "outputs": [],
      "source": [
        "# Turn pretty print back on \n",
        "%pprint\n",
        "# sorted() returns an alphabetically/numerically sorted list\n",
        "sorted(speech_stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How can we write the same thing, but with a list comprehension, you ask??"
      ],
      "metadata": {
        "id": "rikyrpNd_-C-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MT9IFTAy5Wtp"
      },
      "outputs": [],
      "source": [
        "speech_stats2 = [(len(inaug.words(f))/len(inaug.sents(f)), f) for f in inaug.fileids()]\n",
        "sorted(speech_stats2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Doing more things with NLTK\n",
        "\n",
        "Find the size of the corpus (aka, the length of the words list):"
      ],
      "metadata": {
        "id": "YWYpFLG1AFu5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f9F57Bt5Wtq"
      },
      "outputs": [],
      "source": [
        "# Corpus size in number of words\n",
        "len(inaug.words())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can build a word frequency distribution of the words in the corpus, too -- these are all things we did on Monday."
      ],
      "metadata": {
        "id": "9jwfM3RxAOjP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1W-XMQe5Wtq"
      },
      "outputs": [],
      "source": [
        "# Build word frequency distribution for the entire corpus\n",
        "inaug_freq = nltk.FreqDist(inaug.words())\n",
        "inaug_freq.most_common(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tygGNKfg5Wtq"
      },
      "source": [
        "### Your turn\n",
        "\n",
        "- Explore the corpus! \n",
        "- Are the following words getting more or less frequent: 'we', 'the', 'America', 'people'?\n",
        "- Are _words_ themselves getting longer or shorter? Hint: use `sum([1, 2, 3, 4])`\n",
        "- Can you print plots of these summaries?\n",
        "- What do our n-gram statistics look like?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Y8jtWz05Wtr"
      },
      "outputs": [],
      "source": [
        "shake = \"shall i compare thee to a summer's day\".split()\n",
        "shake"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a list of the word lengths"
      ],
      "metadata": {
        "id": "ckf0uIVyBqns"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eOCNL4I5Wtr"
      },
      "outputs": [],
      "source": [
        "[len(word) for word in shake]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What's this doing? Make sure you can explain it!"
      ],
      "metadata": {
        "id": "I7YW2SQ8Bump"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9CJgZnl5Wtr"
      },
      "outputs": [],
      "source": [
        "sum([len(word) for word in shake])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qeQX6-X5Wts"
      },
      "outputs": [],
      "source": [
        "len(shake)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you describe what this line of code is doing?"
      ],
      "metadata": {
        "id": "5QwFPTgVB-F4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pb_W3szW5Wts"
      },
      "outputs": [],
      "source": [
        "sum([len(word) for word in shake]) / len(shake)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an example for how you might find the average word length...\n",
        "\n",
        "Look at the last line here -- what is this sep=\"\\t\" doing in the print statement? What happens if you change it?"
      ],
      "metadata": {
        "id": "BhgLT5Z4CDb1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pm-xASe5Wts"
      },
      "outputs": [],
      "source": [
        "# Average word length. \n",
        "# Trending DOWN! \n",
        "for file in inaug.fileids():\n",
        "    wcount = len(inaug.words(file))\n",
        "    wlen_sum = sum([len(x) for x in inaug.words(file)])\n",
        "    wlen_avg = wlen_sum/wcount\n",
        "    print(wlen_avg, file, sep=\"\\t\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at what happens when you look at the frequency distribution for the word 'we' -- can you draw any conclusions about this? \n",
        "\n",
        "What about any of the other words mentioned, or other words you try?"
      ],
      "metadata": {
        "id": "pXG-YRRfCXO-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRttlOfb5Wts"
      },
      "outputs": [],
      "source": [
        "# 'we' and related forms.  \n",
        "for file in inaug.fileids():\n",
        "    fd = nltk.FreqDist(inaug.words(file))  # frequency distribution for each speech\n",
        "    wecount = fd.freq(\"We\") + fd.freq(\"we\") + fd.freq(\"us\") + fd.freq(\"Our\") + fd.freq(\"our\")\n",
        "    print(wecount, file, sep=\"\\t\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "colab": {
      "name": "day3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}